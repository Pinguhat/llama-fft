10.01.2026

analyze_llama.py:

Loading model: /home/lukas/models/Llama-2-7b-hf
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 39.27it/s]
Listing all nn.Linear layers in the model:

   # | name                                                         |     in ->    out |       params |   MACs/token
--------------------------------------------------------------------------------------------------------------
   1 | model.layers.0.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
   2 | model.layers.0.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
   3 | model.layers.0.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
   4 | model.layers.0.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
   5 | model.layers.0.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
   6 | model.layers.0.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
   7 | model.layers.0.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
   8 | model.layers.1.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
   9 | model.layers.1.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  10 | model.layers.1.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  11 | model.layers.1.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  12 | model.layers.1.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  13 | model.layers.1.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  14 | model.layers.1.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  15 | model.layers.2.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  16 | model.layers.2.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  17 | model.layers.2.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  18 | model.layers.2.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  19 | model.layers.2.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  20 | model.layers.2.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  21 | model.layers.2.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  22 | model.layers.3.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  23 | model.layers.3.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  24 | model.layers.3.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  25 | model.layers.3.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  26 | model.layers.3.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  27 | model.layers.3.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  28 | model.layers.3.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  29 | model.layers.4.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  30 | model.layers.4.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  31 | model.layers.4.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  32 | model.layers.4.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  33 | model.layers.4.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  34 | model.layers.4.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  35 | model.layers.4.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  36 | model.layers.5.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  37 | model.layers.5.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  38 | model.layers.5.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  39 | model.layers.5.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  40 | model.layers.5.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  41 | model.layers.5.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  42 | model.layers.5.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  43 | model.layers.6.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  44 | model.layers.6.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  45 | model.layers.6.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  46 | model.layers.6.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  47 | model.layers.6.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  48 | model.layers.6.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  49 | model.layers.6.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  50 | model.layers.7.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  51 | model.layers.7.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  52 | model.layers.7.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  53 | model.layers.7.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  54 | model.layers.7.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  55 | model.layers.7.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  56 | model.layers.7.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  57 | model.layers.8.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  58 | model.layers.8.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  59 | model.layers.8.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  60 | model.layers.8.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  61 | model.layers.8.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  62 | model.layers.8.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  63 | model.layers.8.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  64 | model.layers.9.self_attn.q_proj                              |   4096 ->   4096 |     16777216 |     16777216
  65 | model.layers.9.self_attn.k_proj                              |   4096 ->   4096 |     16777216 |     16777216
  66 | model.layers.9.self_attn.v_proj                              |   4096 ->   4096 |     16777216 |     16777216
  67 | model.layers.9.self_attn.o_proj                              |   4096 ->   4096 |     16777216 |     16777216
  68 | model.layers.9.mlp.gate_proj                                 |   4096 ->  11008 |     45088768 |     45088768
  69 | model.layers.9.mlp.up_proj                                   |   4096 ->  11008 |     45088768 |     45088768
  70 | model.layers.9.mlp.down_proj                                 |  11008 ->   4096 |     45088768 |     45088768
  71 | model.layers.10.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
  72 | model.layers.10.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
  73 | model.layers.10.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
  74 | model.layers.10.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
  75 | model.layers.10.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
  76 | model.layers.10.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
  77 | model.layers.10.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
  78 | model.layers.11.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
  79 | model.layers.11.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
  80 | model.layers.11.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
  81 | model.layers.11.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
  82 | model.layers.11.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
  83 | model.layers.11.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
  84 | model.layers.11.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
  85 | model.layers.12.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
  86 | model.layers.12.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
  87 | model.layers.12.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
  88 | model.layers.12.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
  89 | model.layers.12.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
  90 | model.layers.12.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
  91 | model.layers.12.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
  92 | model.layers.13.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
  93 | model.layers.13.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
  94 | model.layers.13.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
  95 | model.layers.13.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
  96 | model.layers.13.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
  97 | model.layers.13.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
  98 | model.layers.13.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
  99 | model.layers.14.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 100 | model.layers.14.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 101 | model.layers.14.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 102 | model.layers.14.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 103 | model.layers.14.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 104 | model.layers.14.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 105 | model.layers.14.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 106 | model.layers.15.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 107 | model.layers.15.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 108 | model.layers.15.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 109 | model.layers.15.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 110 | model.layers.15.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 111 | model.layers.15.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 112 | model.layers.15.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 113 | model.layers.16.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 114 | model.layers.16.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 115 | model.layers.16.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 116 | model.layers.16.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 117 | model.layers.16.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 118 | model.layers.16.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 119 | model.layers.16.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 120 | model.layers.17.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 121 | model.layers.17.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 122 | model.layers.17.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 123 | model.layers.17.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 124 | model.layers.17.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 125 | model.layers.17.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 126 | model.layers.17.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 127 | model.layers.18.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 128 | model.layers.18.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 129 | model.layers.18.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 130 | model.layers.18.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 131 | model.layers.18.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 132 | model.layers.18.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 133 | model.layers.18.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 134 | model.layers.19.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 135 | model.layers.19.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 136 | model.layers.19.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 137 | model.layers.19.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 138 | model.layers.19.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 139 | model.layers.19.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 140 | model.layers.19.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 141 | model.layers.20.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 142 | model.layers.20.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 143 | model.layers.20.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 144 | model.layers.20.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 145 | model.layers.20.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 146 | model.layers.20.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 147 | model.layers.20.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 148 | model.layers.21.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 149 | model.layers.21.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 150 | model.layers.21.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 151 | model.layers.21.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 152 | model.layers.21.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 153 | model.layers.21.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 154 | model.layers.21.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 155 | model.layers.22.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 156 | model.layers.22.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 157 | model.layers.22.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 158 | model.layers.22.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 159 | model.layers.22.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 160 | model.layers.22.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 161 | model.layers.22.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 162 | model.layers.23.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 163 | model.layers.23.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 164 | model.layers.23.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 165 | model.layers.23.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 166 | model.layers.23.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 167 | model.layers.23.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 168 | model.layers.23.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 169 | model.layers.24.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 170 | model.layers.24.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 171 | model.layers.24.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 172 | model.layers.24.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 173 | model.layers.24.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 174 | model.layers.24.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 175 | model.layers.24.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 176 | model.layers.25.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 177 | model.layers.25.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 178 | model.layers.25.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 179 | model.layers.25.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 180 | model.layers.25.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 181 | model.layers.25.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 182 | model.layers.25.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 183 | model.layers.26.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 184 | model.layers.26.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 185 | model.layers.26.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 186 | model.layers.26.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 187 | model.layers.26.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 188 | model.layers.26.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 189 | model.layers.26.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 190 | model.layers.27.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 191 | model.layers.27.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 192 | model.layers.27.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 193 | model.layers.27.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 194 | model.layers.27.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 195 | model.layers.27.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 196 | model.layers.27.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 197 | model.layers.28.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 198 | model.layers.28.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 199 | model.layers.28.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 200 | model.layers.28.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 201 | model.layers.28.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 202 | model.layers.28.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 203 | model.layers.28.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 204 | model.layers.29.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 205 | model.layers.29.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 206 | model.layers.29.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 207 | model.layers.29.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 208 | model.layers.29.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 209 | model.layers.29.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 210 | model.layers.29.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 211 | model.layers.30.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 212 | model.layers.30.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 213 | model.layers.30.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 214 | model.layers.30.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 215 | model.layers.30.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 216 | model.layers.30.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 217 | model.layers.30.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 218 | model.layers.31.self_attn.q_proj                             |   4096 ->   4096 |     16777216 |     16777216
 219 | model.layers.31.self_attn.k_proj                             |   4096 ->   4096 |     16777216 |     16777216
 220 | model.layers.31.self_attn.v_proj                             |   4096 ->   4096 |     16777216 |     16777216
 221 | model.layers.31.self_attn.o_proj                             |   4096 ->   4096 |     16777216 |     16777216
 222 | model.layers.31.mlp.gate_proj                                |   4096 ->  11008 |     45088768 |     45088768
 223 | model.layers.31.mlp.up_proj                                  |   4096 ->  11008 |     45088768 |     45088768
 224 | model.layers.31.mlp.down_proj                                |  11008 ->   4096 |     45088768 |     45088768
 225 | lm_head                                                      |   4096 ->  32000 |    131072000 |    131072000

Summary (from real model):
Total nn.Linear layers:           225
Total parameters in nn.Linear:    6,607,077,376
Total MACs per token (all Lin.):  6,607,077,376
Approx FLOPs per token (Lin.):    13,214,154,752
  (FLOPs ~= 2 * MACs: mul + add)


------------------------------------------------
compare_original_and_fft.py

Loading original model...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 83.40it/s]
Loading FFT-patched model...
Loading checkpoint shards: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 85.59it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
    Replacing up_proj: in_features=4096, out_features=11008
    Replacing down_proj: in_features=11008, out_features=4096
L2-Abstand der Logits (Original vs. FFT-MLP): 4.69140625

------------------------------------------------

compare_original_and_fft_tokens.py

Loading original model...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 76.55it/s]
Original predicted token ids: [[917, 4015, 29956, 1199, 9954, 280, 348, 11841, 1005, 762, 265, 744, 12670, 911, 13, 1011]]

Loading FFT-patched model...
Loading checkpoint shards: 100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 69.94it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
    Replacing up_proj: in_features=4096, out_features=11008
    Replacing down_proj: in_features=11008, out_features=4096
FFT-MLP predicted token ids: [[7228, 7228, 29943, 5570, 373, 7228, 13, 13, 13, 13, 293, 13, 13, 13, 13, 1011]]

Same predicted tokens: 2 / 16

Next-token prediction (based on last position):
  Original top-1 id  : 1011 | token: 'ein'
  FFT-MLP top-1 id   : 1011 | token: 'ein'

------------------------------------------------

estimate_fft_savinf.py

d_model = 4096, d_ff = 11008, block_size = 256
Dense linear (4096 -> 11008): 45088768 MACs pro Token

Block-circulant mit FFT (4096 -> 11008):
  ~21839872 Multiplikationen pro Token
  ~21487616 Additionen pro Token

Vergleich pro Linear-Layer:
  Dense mult : 45088768
  FFT  mult  : 21839872
  Speedup (mult) ~ 2.06x

Pro MLP-Block (gate, up, down):
  Dense MACs (approx) : 135266304
  FFT mult  (approx)  : 65519616
  FFT add   (approx)  : 64462848
  Speedup (mult) ~ 2.06x

------------------------------------------------
