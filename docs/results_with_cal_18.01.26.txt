---------------------------
Without calibration
---------------------------
compare_original_and_fft.py
Loading original model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 29.27it/s]
Loading FFT-patched model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50.68it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing up_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing down_proj: in_features=11008, out_features=4096
      Using circulant convention: diag_inv
Texts tested: 20
L2 mean/std: 3.8033204078674316 0.5472416877746582
KL mean/std: 1.2213776111602783 0.8648613095283508
Cos mean/std: 0.7520443797111511 0.4424303472042084
Top-1 next-token match: 10 / 20
Top-5 next-token overlap mean/std: 2.25 0.9665456414222717

----------------------------------------------------------------------------------------------

compare_original_and_fft_tokens.py
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.93it/s]

Loading FFT-patched model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.80it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing up_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing down_proj: in_features=11008, out_features=4096
      Using circulant convention: diag_inv

Text: 'Explain in one sentence why approximation errors can accumulate in deep networks.'
  Same tokens: 8 / 16
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Complete the sentence: A block-circulant projection changes the model because'
  Same tokens: 4 / 16
  Next-token ids: 372 vs 278
  Next-token: 'it' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Continue: When logits shift slightly, the most likely token can'
  Same tokens: 5 / 15
  Next-token ids: 1735 vs 367
  Next-token: 'change' vs 'be'
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 2 / 5
  Patched top-k: ["'be'", "'become'", "'have'", '"\'"', "'also'"]

Text: 'Finish this thought: Using FFTs helps mainly by reducing'
  Same tokens: 4 / 14
  Next-token ids: 278 vs 278
  Next-token: 'the' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'In plain words, describe what KL divergence measures for next-token distributions.'
  Same tokens: 7 / 18
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 1 / 5

Text: 'Continue: A practical trade-off in model compression is'
  Same tokens: 6 / 13
  Next-token ids: 304 vs 278
  Next-token: 'to' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Write the next phrase: The main advantage of structured matrices is'
  Same tokens: 8 / 14
  Next-token ids: 393 vs 393
  Next-token: 'that' vs 'that'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Complete: If top-1 changes but top-5 stays similar, then'
  Same tokens: 6 / 16
  Next-token ids: 278 vs 278
  Next-token: 'the' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Continue: One reason cosine similarity is useful is that it'
  Same tokens: 5 / 14
  Next-token ids: 338 vs 6511
  Next-token: 'is' vs 'allows'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Finish the sentence: Calibration after approximation is similar to'
  Same tokens: 6 / 13
  Next-token ids: 903 vs 903
  Next-token: '_' vs '_'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Continue: In transformers, the MLP layers are important because they'
  Same tokens: 7 / 16
  Next-token ids: 526 vs 526
  Next-token: 'are' vs 'are'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Complete: The risk of aggressive compression is that it may'
  Same tokens: 5 / 14
  Next-token ids: 4556 vs 367
  Next-token: 'cause' vs 'be'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Continue: A simple sanity check for a patched model is'
  Same tokens: 5 / 15
  Next-token ids: 304 vs 451
  Next-token: 'to' vs 'not'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Finish this thought: The difference between L2 and KL on logits is'
  Same tokens: 8 / 18
  Next-token ids: 393 vs 393
  Next-token: 'that' vs 'that'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 1 / 5

Text: 'Continue: If the teacher distribution is sharp, small perturbations can'
  Same tokens: 8 / 15
  Next-token ids: 4556 vs 367
  Next-token: 'cause' vs 'be'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Vervollständige: Kleine Logit-Verschiebungen können dazu führen, dass'
  Same tokens: 6 / 22
  Next-token ids: 762 vs 762
  Next-token: 'die' vs 'die'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Führe fort: Ein Vorteil strukturierter Gewichtsmatrizen ist, dass'
  Same tokens: 4 / 21
  Next-token ids: 2686 vs 29892
  Next-token: 'sie' vs ','
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 0 / 5
  Patched top-k: ["','", "':'", "'the'", "'a'", "''"]

Text: 'Erkläre kurz: Wieso kann sich ein Fehler in frühen Layern später verstärken?'
  Same tokens: 7 / 26
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Vervollständige: Eine Kalibrierung nach der Approximation soll erreichen, dass'
  Same tokens: 5 / 22
  Next-token ids: 762 vs 29892
  Next-token: 'die' vs ','
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Führe fort: Ein einfacher Test für die Ähnlichkeit zweier Modelle ist'
  Same tokens: 6 / 21
  Next-token ids: 762 vs 1011
  Next-token: 'die' vs 'ein'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Overall token agreement: 120 / 339
Overall next-token agreement: 10 / 20
Overall next-token top-5 agreement: 18 / 20
Per-text token agreement mean/std: 0.36678799986839294 0.10638249665498734
Next-token top-5 overlap mean/std: 2.25 0.9665456414222717












---------------------------
With calibration "bc_calibrated.pt"
---------------------------
Calibration very low:

calibrate_bc.py --prompts_file llama-fft/src/prompts_20_quality.txt --limit 20 --steps 25 --lr 1e-3 --out llama-fft/src/bc_calibrated.pt

compare_original_and_fft.py
Loading original model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36.53it/s]
Loading FFT-patched model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 49.80it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing up_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing down_proj: in_features=11008, out_features=4096
      Using circulant convention: diag_inv
Loaded BC params from llama-fft/src/bc_calibrated.pt: loaded=3, skipped=0
Texts tested: 20
L2 mean/std: 3.7103514671325684 0.6262512803077698
KL mean/std: 1.2287652492523193 0.9822441339492798
Cos mean/std: 0.7597737312316895 0.4400031864643097
Top-1 next-token match: 12 / 20
Top-5 next-token overlap mean/std: 2.299999952316284 1.3018206357955933



-----------------------------------------------------------------------------------------------

compare_original_and_fft_tokens.py

Loading original model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47.43it/s]

Loading FFT-patched model...
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 46.40it/s]
Model has 32 transformer layers.
Patching the first 1 layer(s).
  Patching layer 0 MLP...
    Replacing gate_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing up_proj: in_features=4096, out_features=11008
      Using circulant convention: diag_inv
    Replacing down_proj: in_features=11008, out_features=4096
      Using circulant convention: diag_inv
Loaded BC params from llama-fft/src/bc_calibrated.pt: loaded=3, skipped=0

Text: 'Explain in one sentence why approximation errors can accumulate in deep networks.'
  Same tokens: 12 / 16
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Complete the sentence: A block-circulant projection changes the model because'
  Same tokens: 4 / 16
  Next-token ids: 372 vs 372
  Next-token: 'it' vs 'it'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Continue: When logits shift slightly, the most likely token can'
  Same tokens: 6 / 15
  Next-token ids: 1735 vs 367
  Next-token: 'change' vs 'be'
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 2 / 5
  Patched top-k: ["'be'", "'continue'", '"\'"', "'shift'", "'move'"]

Text: 'Finish this thought: Using FFTs helps mainly by reducing'
  Same tokens: 5 / 14
  Next-token ids: 278 vs 278
  Next-token: 'the' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'In plain words, describe what KL divergence measures for next-token distributions.'
  Same tokens: 7 / 18
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 1 / 5

Text: 'Continue: A practical trade-off in model compression is'
  Same tokens: 7 / 13
  Next-token ids: 304 vs 263
  Next-token: 'to' vs 'a'
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 1 / 5
  Patched top-k: ["'a'", "'the'", "'not'", "'an'", "'used'"]

Text: 'Write the next phrase: The main advantage of structured matrices is'
  Same tokens: 9 / 14
  Next-token ids: 393 vs 393
  Next-token: 'that' vs 'that'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Complete: If top-1 changes but top-5 stays similar, then'
  Same tokens: 10 / 16
  Next-token ids: 278 vs 278
  Next-token: 'the' vs 'the'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Continue: One reason cosine similarity is useful is that it'
  Same tokens: 7 / 14
  Next-token ids: 338 vs 338
  Next-token: 'is' vs 'is'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Finish the sentence: Calibration after approximation is similar to'
  Same tokens: 7 / 13
  Next-token ids: 903 vs 903
  Next-token: '_' vs '_'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Continue: In transformers, the MLP layers are important because they'
  Same tokens: 8 / 16
  Next-token ids: 526 vs 526
  Next-token: 'are' vs 'are'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Complete: The risk of aggressive compression is that it may'
  Same tokens: 8 / 14
  Next-token ids: 4556 vs 4556
  Next-token: 'cause' vs 'cause'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Continue: A simple sanity check for a patched model is'
  Same tokens: 5 / 15
  Next-token ids: 304 vs 451
  Next-token: 'to' vs 'not'
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 1 / 5
  Patched top-k: ["'not'", "'a'", "'used'", "'performed'", "'required'"]

Text: 'Finish this thought: The difference between L2 and KL on logits is'
  Same tokens: 9 / 18
  Next-token ids: 393 vs 393
  Next-token: 'that' vs 'that'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Continue: If the teacher distribution is sharp, small perturbations can'
  Same tokens: 8 / 15
  Next-token ids: 4556 vs 367
  Next-token: 'cause' vs 'be'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 4 / 5

Text: 'Vervollständige: Kleine Logit-Verschiebungen können dazu führen, dass'
  Same tokens: 2 / 22
  Next-token ids: 762 vs 29892
  Next-token: 'die' vs ','
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 2 / 5

Text: 'Führe fort: Ein Vorteil strukturierter Gewichtsmatrizen ist, dass'
  Same tokens: 3 / 21
  Next-token ids: 2686 vs 29892
  Next-token: 'sie' vs ','
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 0 / 5
  Patched top-k: ["','", "':'", "'.'", "''", "'\\n'"]

Text: 'Erkläre kurz: Wieso kann sich ein Fehler in frühen Layern später verstärken?'
  Same tokens: 5 / 26
  Next-token ids: 13 vs 13
  Next-token: '\n' vs '\n'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Text: 'Vervollständige: Eine Kalibrierung nach der Approximation soll erreichen, dass'
  Same tokens: 2 / 22
  Next-token ids: 762 vs 29892
  Next-token: 'die' vs ','
  Next-token in top-5 (patched): False
  Next-token top-5 overlap (orig vs patched): 0 / 5
  Patched top-k: ["','", "':'", "'\\n'", "'.'", "''"]

Text: 'Führe fort: Ein einfacher Test für die Ähnlichkeit zweier Modelle ist'
  Same tokens: 4 / 21
  Next-token ids: 762 vs 1011
  Next-token: 'die' vs 'ein'
  Next-token in top-5 (patched): True
  Next-token top-5 overlap (orig vs patched): 3 / 5

Overall token agreement: 128 / 339
Overall next-token agreement: 12 / 20
Overall next-token top-5 agreement: 15 / 20
Per-text token agreement mean/std: 0.40681833028793335 0.19464154541492462
Next-token top-5 overlap mean/std: 2.299999952316284 1.3018206357955933

