{
  "results": {
    "leaderboard_bbh_boolean_expressions": {
      "alias": "leaderboard_bbh_boolean_expressions",
      "acc_norm,none": 0.37,
      "acc_norm_stderr,none": 0.048523658709390974
    },
    "leaderboard_bbh_date_understanding": {
      "alias": "leaderboard_bbh_date_understanding",
      "acc_norm,none": 0.16,
      "acc_norm_stderr,none": 0.03684529491774706
    },
    "leaderboard_bbh_object_counting": {
      "alias": "leaderboard_bbh_object_counting",
      "acc_norm,none": 0.09,
      "acc_norm_stderr,none": 0.028762349126466143
    },
    "leaderboard_gpqa_main": {
      "alias": "leaderboard_gpqa_main",
      "acc_norm,none": 0.24,
      "acc_norm_stderr,none": 0.04292346959909278
    },
    "leaderboard_mmlu_pro": {
      "alias": "leaderboard_mmlu_pro",
      "acc,none": 0.14,
      "acc_stderr,none": 0.0348735088019777
    }
  },
  "group_subtasks": {
    "leaderboard_bbh_boolean_expressions": [],
    "leaderboard_bbh_date_understanding": [],
    "leaderboard_bbh_object_counting": [],
    "leaderboard_gpqa_main": [],
    "leaderboard_mmlu_pro": []
  },
  "configs": {
    "leaderboard_bbh_boolean_expressions": {
      "task": "leaderboard_bbh_boolean_expressions",
      "dataset_path": "SaylorTwift/bbh",
      "dataset_name": "boolean_expressions",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "doc_to_choice": [
        "False",
        "True"
      ],
      "description": "Evaluate the result of a random Boolean expression.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "samples": [
          {
            "input": "not ( ( not not True ) ) is",
            "target": "False"
          },
          {
            "input": "True and False and not True and True is",
            "target": "False"
          },
          {
            "input": "not not ( not ( False ) ) is",
            "target": "True"
          }
        ]
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "leaderboard_bbh_date_understanding": {
      "task": "leaderboard_bbh_date_understanding",
      "dataset_path": "SaylorTwift/bbh",
      "dataset_name": "date_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)",
        "(E)",
        "(F)"
      ],
      "description": "Infer the date from context.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "samples": [
          {
            "input": "Today is Christmas Eve of 1937. What is the date 10 days ago in MM/DD/YYYY?\nOptions:\n(A) 12/14/2026\n(B) 12/14/1950\n(C) 12/14/2007\n(D) 12/14/1937\n(E) 07/14/1938\n(F) 12/14/1988",
            "target": "(D)"
          },
          {
            "input": "Tomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY?\nOptions:\n(A) 09/04/2018\n(B) 11/11/2018\n(C) 08/25/2018\n(D) 11/02/2018\n(E) 11/04/2018",
            "target": "(B)"
          },
          {
            "input": "Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date tomorrow in MM/DD/YYYY?\nOptions:\n(A) 01/11/1961\n(B) 01/03/1963\n(C) 01/18/1961\n(D) 10/14/1960\n(E) 01/03/1982\n(F) 12/03/1960",
            "target": "(B)"
          }
        ]
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "leaderboard_bbh_object_counting": {
      "task": "leaderboard_bbh_object_counting",
      "dataset_path": "SaylorTwift/bbh",
      "dataset_name": "object_counting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "doc_to_choice": [
        "0",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "10",
        "11",
        "12",
        "13",
        "14",
        "15",
        "16",
        "17",
        "18"
      ],
      "description": "Questions that involve enumerating objects and asking the model to count them.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "samples": [
          {
            "input": "I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?",
            "target": "6"
          },
          {
            "input": "I have an orange, a raspberry, two peaches, a blackberry, an apple, a grape, a nectarine, and three plums. How many fruits do I have?",
            "target": "11"
          },
          {
            "input": "I have a lettuce head, a head of broccoli, an onion, a stalk of celery, two carrots, a garlic, and a yam. How many vegetables do I have?",
            "target": "8"
          }
        ]
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "leaderboard_gpqa_main": {
      "task": "leaderboard_gpqa_main",
      "dataset_path": "Idavidrein/gpqa",
      "dataset_name": "gpqa_main",
      "training_split": "train",
      "validation_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        choices = [\n            preprocess(doc[\"Incorrect Answer 1\"]),\n            preprocess(doc[\"Incorrect Answer 2\"]),\n            preprocess(doc[\"Incorrect Answer 3\"]),\n            preprocess(doc[\"Correct Answer\"]),\n        ]\n\n        random.shuffle(choices)\n        correct_answer_index = choices.index(preprocess(doc[\"Correct Answer\"]))\n\n        out_doc = {\n            \"choice1\": choices[0],\n            \"choice2\": choices[1],\n            \"choice3\": choices[2],\n            \"choice4\": choices[3],\n            \"answer\": f\"({chr(65 + correct_answer_index)})\",\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
      "doc_to_text": "What is the correct answer to this question:{{Question}}\nChoices:\n(A) {{choice1}}\n(B) {{choice2}}\n(C) {{choice3}}\n(D) {{choice4}}\nAnswer: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "leaderboard_mmlu_pro": {
      "task": "leaderboard_mmlu_pro",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "def doc_to_text(doc):\n    doc_to_text = f\"{doc['question']}\\n\"\n\n    for i in range(len(doc[\"options\"])):\n        doc_to_text += f\"{string.ascii_uppercase[i]}. {doc['options'][i]}\\n\"\n\n    doc_to_text += \"Answer:\"\n    return doc_to_text\n",
      "doc_to_target": "answer",
      "doc_to_choice": "def doc_to_choice(doc):\n    return [string.ascii_uppercase[i] for i in range(len(doc[\"options\"]))]\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.1
      }
    }
  },
  "versions": {
    "leaderboard_bbh_boolean_expressions": 0.0,
    "leaderboard_bbh_date_understanding": 0.0,
    "leaderboard_bbh_object_counting": 0.0,
    "leaderboard_gpqa_main": 1.0,
    "leaderboard_mmlu_pro": 0.1
  },
  "n-shot": {
    "leaderboard_bbh_boolean_expressions": 3,
    "leaderboard_bbh_date_understanding": 3,
    "leaderboard_bbh_object_counting": 3,
    "leaderboard_gpqa_main": 0,
    "leaderboard_mmlu_pro": 5
  },
  "higher_is_better": {
    "leaderboard_bbh_boolean_expressions": {
      "acc_norm": true
    },
    "leaderboard_bbh_date_understanding": {
      "acc_norm": true
    },
    "leaderboard_bbh_object_counting": {
      "acc_norm": true
    },
    "leaderboard_gpqa_main": {
      "acc_norm": true
    },
    "leaderboard_mmlu_pro": {
      "acc": true
    }
  },
  "n-samples": {
    "leaderboard_mmlu_pro": {
      "original": 12032,
      "effective": 100
    },
    "leaderboard_gpqa_main": {
      "original": 448,
      "effective": 100
    },
    "leaderboard_bbh_object_counting": {
      "original": 250,
      "effective": 100
    },
    "leaderboard_bbh_date_understanding": {
      "original": 250,
      "effective": 100
    },
    "leaderboard_bbh_boolean_expressions": {
      "original": 250,
      "effective": 100
    }
  },
  "config": {
    "model": "llama_fft",
    "model_args": "pretrained=C:/Users/Lukas/Documents/0-UNI/Seminar/Llama2,dtype=float16,device=cuda,use_cache=False,block_size=64,num_layers=8,calib_path=C:/Users/Lukas/Documents/0-UNI/Seminar/LLAMA-FFT/src/calib_out_long_L50_T128_steps500_L8/bc_calibrated_B64.pt,cache_cfft=1",
    "model_num_parameters": 5673193472,
    "model_dtype": "torch.float16",
    "model_revision": "main",
    "model_sha": "",
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": 100.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "3c26a58",
  "date": 1771412194.8057194,
  "pretty_env_info": "PyTorch version: 2.9.1+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Microsoft Windows 11 Education (10.0.26200 64-Bit)\nGCC version: (MinGW.org GCC Build-2) 9.2.0\r\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.13.11 (tags/v3.13.11:6278944, Dec  5 2025, 16:26:58) [MSC v.1944 64 bit (AMD64)] (64-bit runtime)\nPython platform: Windows-11-10.0.26200-SP0\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: \nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060 SUPER\nNvidia driver version: 591.59\ncuDNN version: Could not collect\nIs XPU available: False\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nName: AMD Ryzen 5 3600 6-Core Processor              \nManufacturer: AuthenticAMD\nFamily: 107\nArchitecture: 9\nProcessorType: 3\nDeviceID: CPU0\nCurrentClockSpeed: 3600\nMaxClockSpeed: 3600\nL2CacheSize: 3072\nL2CacheSpeed: None\nRevision: 28928\n\nVersions of relevant libraries:\n[pip3] numpy==2.3.5\n[pip3] torch==2.9.1+cu128\n[pip3] torchaudio==2.9.1+cu128\n[pip3] torchvision==0.24.1+cu128\n[conda] Could not collect",
  "transformers_version": "4.57.6",
  "upper_git_hash": "3c26a58d47eaede00d3e92d76c67b3466b70ed24",
  "tokenizer_pad_token": [
    "<unk>",
    "0"
  ],
  "tokenizer_eos_token": [
    "</s>",
    "2"
  ],
  "tokenizer_bos_token": [
    "<s>",
    "1"
  ],
  "eot_token_id": 2,
  "max_length": 4096,
  "task_hashes": {},
  "model_source": "llama_fft",
  "model_name": "C:/Users/Lukas/Documents/0-UNI/Seminar/Llama2",
  "model_name_sanitized": "C__Users__Lukas__Documents__0-UNI__Seminar__Llama2",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 441851.1983018,
  "end_time": 449225.9795318,
  "total_evaluation_time_seconds": "7374.781230000022"
}